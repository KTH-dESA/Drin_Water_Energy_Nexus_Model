{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "suitable-stable",
   "metadata": {},
   "source": [
    "# DRIN Preprocessing Script1 - SMHI climate projections\n",
    "\n",
    "This notebook is intended to process the climate projection data from the Swedish Metrological and Hydrological Institute (SMHI) in order to develop water flow in the Drin river segments. \n",
    "\n",
    "Note: To be able to run this notebook you you have the required raw data under 'data/smhi_data'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-comparison",
   "metadata": {},
   "source": [
    "### Import packages and dependencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dental-enzyme",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"..\") #this is to add the above folder to the package directory\n",
    "import os\n",
    "import datetime as dt\n",
    "import glob\n",
    "import re\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-lending",
   "metadata": {},
   "source": [
    "# Part 1: Historical flow (Reference Scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-delaware",
   "metadata": {},
   "source": [
    "### 1.1 Seting up the dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "broke-practice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the folder and the scenario path\n",
    "\n",
    "folder = os.path.join('..','Data', 'smhi_data')\n",
    "scenario_path = os.path.join(folder ,'ref')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "subtle-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the 'raw_*.csv', performing some changes and saving it as 'pros_*.csv':\n",
    "\n",
    "for filepath in glob.iglob(os.path.join(scenario_path, ('raw_*.csv'))):\n",
    "    #print(filepath)\n",
    "    df=pd.read_csv(filepath)\n",
    "    i=filepath[26:31]\n",
    "    df['catchment']= i\n",
    "    df['scenario']= 'REF'\n",
    "    df['model']='e_hype'\n",
    "    df.rename({'River discharge (m3/s)':'value'}, axis=1, inplace=True)\n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "    df['Date'] = pd.to_datetime(df[\"Date\"].dt.strftime('%d/%m/%Y'))\n",
    "    df=df[['model','catchment','scenario','Date','value']]\n",
    "    \n",
    "    ##print(i)\n",
    "    #df.to_csv(os.path.join(scenario_path, 'pros_' + str(i) + '.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "featured-question",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the 'pros_*.csv' files:\n",
    "joined_files = os.path.join(scenario_path, \"pros_*.csv\")\n",
    "  \n",
    "# A list of all joined files is returned\n",
    "joined_list = glob.glob(joined_files)\n",
    "  \n",
    "# Finally, the files are joined in one dataframe:\n",
    "df = pd.concat(map(pd.read_csv, joined_list), ignore_index=True)\n",
    "\n",
    "df2 = pd.pivot_table(df, values='value', index=['Date'],\n",
    "                    columns=['catchment'], aggfunc=np.sum).round(3).reset_index()\n",
    "\n",
    "# Save the file:\n",
    "#df2.to_csv(os.path.join(scenario_path,'all_catchments_processed.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "underlying-collectible",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>catchment</th>\n",
       "      <th>black</th>\n",
       "      <th>fierz</th>\n",
       "      <th>globo</th>\n",
       "      <th>koman</th>\n",
       "      <th>ohrid</th>\n",
       "      <th>spilj</th>\n",
       "      <th>vaudj</th>\n",
       "      <th>white</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10957.000000</td>\n",
       "      <td>10957.000000</td>\n",
       "      <td>10957.000000</td>\n",
       "      <td>10957.000000</td>\n",
       "      <td>10957.000000</td>\n",
       "      <td>10957.000000</td>\n",
       "      <td>10957.000000</td>\n",
       "      <td>10957.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>78.099397</td>\n",
       "      <td>169.861543</td>\n",
       "      <td>18.124613</td>\n",
       "      <td>199.440866</td>\n",
       "      <td>5.025431</td>\n",
       "      <td>52.685578</td>\n",
       "      <td>236.833895</td>\n",
       "      <td>56.108966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>83.821065</td>\n",
       "      <td>143.619135</td>\n",
       "      <td>16.230676</td>\n",
       "      <td>163.803115</td>\n",
       "      <td>6.440248</td>\n",
       "      <td>51.220808</td>\n",
       "      <td>189.099657</td>\n",
       "      <td>56.459141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.355000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.109000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.136000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>21.041000</td>\n",
       "      <td>140.066000</td>\n",
       "      <td>6.311000</td>\n",
       "      <td>140.280000</td>\n",
       "      <td>0.497000</td>\n",
       "      <td>19.280000</td>\n",
       "      <td>136.779000</td>\n",
       "      <td>17.956000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>51.526000</td>\n",
       "      <td>140.066000</td>\n",
       "      <td>16.897000</td>\n",
       "      <td>159.733000</td>\n",
       "      <td>2.108000</td>\n",
       "      <td>36.611000</td>\n",
       "      <td>186.007000</td>\n",
       "      <td>38.681000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>104.698000</td>\n",
       "      <td>185.692000</td>\n",
       "      <td>17.611000</td>\n",
       "      <td>237.145000</td>\n",
       "      <td>7.814000</td>\n",
       "      <td>69.990000</td>\n",
       "      <td>304.418000</td>\n",
       "      <td>72.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>846.617000</td>\n",
       "      <td>1208.006000</td>\n",
       "      <td>165.939000</td>\n",
       "      <td>1350.018000</td>\n",
       "      <td>39.646000</td>\n",
       "      <td>462.182000</td>\n",
       "      <td>1518.558000</td>\n",
       "      <td>522.583000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "catchment         black         fierz         globo         koman  \\\n",
       "count      10957.000000  10957.000000  10957.000000  10957.000000   \n",
       "mean          78.099397    169.861543     18.124613    199.440866   \n",
       "std           83.821065    143.619135     16.230676    163.803115   \n",
       "min            0.000000      0.000000      0.355000      0.000000   \n",
       "25%           21.041000    140.066000      6.311000    140.280000   \n",
       "50%           51.526000    140.066000     16.897000    159.733000   \n",
       "75%          104.698000    185.692000     17.611000    237.145000   \n",
       "max          846.617000   1208.006000    165.939000   1350.018000   \n",
       "\n",
       "catchment         ohrid         spilj         vaudj         white  \n",
       "count      10957.000000  10957.000000  10957.000000  10957.000000  \n",
       "mean           5.025431     52.685578    236.833895     56.108966  \n",
       "std            6.440248     51.220808    189.099657     56.459141  \n",
       "min            0.000000      1.109000      0.000000      2.136000  \n",
       "25%            0.497000     19.280000    136.779000     17.956000  \n",
       "50%            2.108000     36.611000    186.007000     38.681000  \n",
       "75%            7.814000     69.990000    304.418000     72.980000  \n",
       "max           39.646000    462.182000   1518.558000    522.583000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-singapore",
   "metadata": {},
   "source": [
    "### NOTE: \n",
    "\n",
    "Checking the dataset shows that the max values occured in these years: 1985, 1986, 2004 and 2010\n",
    "we know that the year 2010 was a flooding years in the region, therefore we will remove this year (2010) from the calculations\n",
    "\n",
    "Fierza max in 2010\n",
    "black max in 1985\n",
    "globo max in 1986\n",
    "koman max in 2010\n",
    "ohrid max in 1986\n",
    "spilj max in 1985\n",
    "vaudj max in 2010\n",
    "white max in 2004\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-tobacco",
   "metadata": {},
   "source": [
    "### 1.2 Restructuring the river segments and introducing the missing catchments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "durable-connecticut",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introducing the missing catchments: (This is related to the structure of the hydrological system in osemosys, which is lightly different than the structure in smhi portal)\n",
    "\n",
    "df2 = df2[['Date', 'ohrid','globo', 'spilj','black', 'white', 'fierz', 'koman', 'vaudj']]\n",
    "\n",
    "df2.rename({\n",
    "#'ohrid':'ohrid_ct',  #ohrid lake outflow\n",
    "#'globo':'BDCT1',      #Inflow from Golbo catchment\n",
    "'globo':'BDRS1',      #Inflow from Golbo catchment\n",
    "'spilj':'BDRS2',      #Inflow in river segment before spilje\n",
    "'black':'BDRS3',      #Inflow in the river segment after spilje and before white drin and skavica\n",
    "'white':'WDRS1',      #Inflow from white drin\n",
    "'fierz':'DRS1',       #Inflow in river segment before fierza HPP\n",
    "'koman':'DRS2',       #Inflow in river segment before koman HPP\n",
    "'vaudj':'DRS3'        #Inflow in river segment before Vau i dejes HPP\n",
    "}, axis=1, inplace=True)\n",
    "\n",
    "#df2['BDRS1'] = df2['ohrid']+df2['BDCT1']  #Inflow in river segment before Globocica\n",
    "df2.loc[df2['BDRS1']>df2['ohrid'], 'BDCT1'] = df2['BDRS1']- df2['ohrid']\n",
    "df2.loc[df2['BDRS1']<=df2['ohrid'], 'BDCT1'] = df2['BDCT1'].min()\n",
    "\n",
    "#Inflow from catchment after Globocica and before Spilje\n",
    "df2.loc[df2['BDRS2']>df2['BDRS1'], 'BDCT2'] = df2['BDRS2']-df2['BDRS1']\n",
    "df2.loc[df2['BDRS2']<=df2['BDRS1'], 'BDCT2'] = df2['BDCT2'].min()\n",
    "\n",
    "#Inflow from catchment after Spilje and before white drin \n",
    "df2.loc[df2['BDRS3']>df2['BDRS2'], 'BDCT3'] = df2['BDRS3']-df2['BDRS2']\n",
    "df2.loc[df2['BDRS3']<=df2['BDRS2'], 'BDCT3'] = df2['BDCT3'].min()\n",
    "\n",
    "\n",
    "#Inflow from catchment before Fierza \n",
    "df2.loc[df2['DRS1']>(df2['BDRS3']+df2['WDRS1']), 'DCT1'] = df2['DRS1']-(df2['BDRS3']+df2['WDRS1'])\n",
    "df2.loc[df2['DRS1']<=(df2['BDRS3']+df2['WDRS1']), 'DCT1'] = df2['WDRS1'].min()\n",
    "\n",
    "#Inflow from catchment before Koman\n",
    "df2.loc[df2['DRS2']>df2['DRS1'], 'DCT2'] = df2['DRS2']-df2['DRS1']\n",
    "df2.loc[df2['DRS2']<=df2['DRS1'], 'DCT2'] = df2['DCT2'].min()\n",
    "\n",
    "#Inflow from catchment before vau i dejes\n",
    "df2.loc[df2['DRS3']>df2['DRS2'], 'DCT3'] = df2['DRS3']-df2['DRS2']\n",
    "df2.loc[df2['DRS3']<=df2['DRS2'], 'DCT3'] = df2['DCT3'].min()\n",
    "            \n",
    "\n",
    "df2 = df2[['Date','ohrid', 'BDCT1','BDRS1','BDCT2', 'BDRS2','BDCT3', 'BDRS3', 'WDRS1', 'DCT1','DRS1','DCT2', 'DRS2','DCT3','DRS3']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automatic-ukraine",
   "metadata": {},
   "source": [
    "### 1.3 Processing smhi reference scenario data to generate timeseries flows\n",
    "\n",
    "#Organizing the time series data of historical flow for the visulaization: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "changed-numbers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To generate a timeseries dataset:\n",
    "\n",
    "dfts= df2.copy()\n",
    "dfts['scenario'] = 'ref'\n",
    "dfts['year'] = pd.DatetimeIndex(dfts['Date']).year\n",
    "dfts = dfts.loc[dfts['year']!=2010]  # To remove the year 2010\n",
    "dfts['model']='ehype'\n",
    "dfts = dfts.melt(id_vars=['Date', 'year','model', 'scenario'], var_name='catchment',value_name='value',ignore_index=True)\n",
    "\n",
    "# Save:\n",
    "output_folder = os.path.join('processed_data','smhi_ref_data')\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "#dfts.to_csv(os.path.join(output_folder, 'ehype_ref_timeseries_allcatchment_b20220211.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-assistant",
   "metadata": {},
   "source": [
    "### 1.4 Processing smhi reference scenario data to generate weekly flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dirty-female",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-77eaa53666a2>:7: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "  dff['week']=pd.DatetimeIndex(dff['Date']).week\n"
     ]
    }
   ],
   "source": [
    "# Aggregate by week and use 2020 as the reference year:\n",
    "\n",
    "dff = df2\n",
    "\n",
    "dff['year'] = pd.DatetimeIndex(dff['Date']).year\n",
    "dff['month']=pd.DatetimeIndex(dff['Date']).month\n",
    "dff['week']=pd.DatetimeIndex(dff['Date']).week\n",
    "dff['day']=pd.DatetimeIndex(dff['Date']).day\n",
    "dff=dff.loc[dff['year']!=2010]\n",
    "dff=dff.groupby('week').mean().round(3).reset_index()\n",
    "dff.drop(['year','month','day'], axis=1, inplace=True)\n",
    "dff['model']='ehype'\n",
    "dff['scenario']='ref'\n",
    "dff['year']='2020'\n",
    "\n",
    "dff_melted = dff.melt(id_vars=['model','scenario','year','week'], var_name='catchment',value_name='value',ignore_index=True)\n",
    "\n",
    "dff_melted = dff_melted[['model','scenario','year','catchment','week','value']]\n",
    "\n",
    "# Saving the output file\n",
    "\n",
    "results_folder = os.path.join('processed_data', 'smhi_ref_data')\n",
    "os.makedirs(results_folder, exist_ok = True)\n",
    "#dff_melted.to_csv(os.path.join(results_folder, 'ehype_ref_weeklyavg_allcatchments_b20220211.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "encouraging-malta",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>scenario</th>\n",
       "      <th>year</th>\n",
       "      <th>catchment</th>\n",
       "      <th>week</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ehype</td>\n",
       "      <td>ref</td>\n",
       "      <td>2020</td>\n",
       "      <td>ohrid</td>\n",
       "      <td>1</td>\n",
       "      <td>6.420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ehype</td>\n",
       "      <td>ref</td>\n",
       "      <td>2020</td>\n",
       "      <td>ohrid</td>\n",
       "      <td>2</td>\n",
       "      <td>3.224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ehype</td>\n",
       "      <td>ref</td>\n",
       "      <td>2020</td>\n",
       "      <td>ohrid</td>\n",
       "      <td>3</td>\n",
       "      <td>6.018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ehype</td>\n",
       "      <td>ref</td>\n",
       "      <td>2020</td>\n",
       "      <td>ohrid</td>\n",
       "      <td>4</td>\n",
       "      <td>6.617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ehype</td>\n",
       "      <td>ref</td>\n",
       "      <td>2020</td>\n",
       "      <td>ohrid</td>\n",
       "      <td>5</td>\n",
       "      <td>7.221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>ehype</td>\n",
       "      <td>ref</td>\n",
       "      <td>2020</td>\n",
       "      <td>DRS3</td>\n",
       "      <td>49</td>\n",
       "      <td>226.816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>ehype</td>\n",
       "      <td>ref</td>\n",
       "      <td>2020</td>\n",
       "      <td>DRS3</td>\n",
       "      <td>50</td>\n",
       "      <td>251.407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>ehype</td>\n",
       "      <td>ref</td>\n",
       "      <td>2020</td>\n",
       "      <td>DRS3</td>\n",
       "      <td>51</td>\n",
       "      <td>320.877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>ehype</td>\n",
       "      <td>ref</td>\n",
       "      <td>2020</td>\n",
       "      <td>DRS3</td>\n",
       "      <td>52</td>\n",
       "      <td>329.575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>ehype</td>\n",
       "      <td>ref</td>\n",
       "      <td>2020</td>\n",
       "      <td>DRS3</td>\n",
       "      <td>53</td>\n",
       "      <td>394.776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>742 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     model scenario  year catchment  week    value\n",
       "0    ehype      ref  2020     ohrid     1    6.420\n",
       "1    ehype      ref  2020     ohrid     2    3.224\n",
       "2    ehype      ref  2020     ohrid     3    6.018\n",
       "3    ehype      ref  2020     ohrid     4    6.617\n",
       "4    ehype      ref  2020     ohrid     5    7.221\n",
       "..     ...      ...   ...       ...   ...      ...\n",
       "737  ehype      ref  2020      DRS3    49  226.816\n",
       "738  ehype      ref  2020      DRS3    50  251.407\n",
       "739  ehype      ref  2020      DRS3    51  320.877\n",
       "740  ehype      ref  2020      DRS3    52  329.575\n",
       "741  ehype      ref  2020      DRS3    53  394.776\n",
       "\n",
       "[742 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff_melted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-variance",
   "metadata": {},
   "source": [
    "### 1.5 Generating flow graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-crowd",
   "metadata": {},
   "source": [
    "### Timeseties flow graphs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "pleasant-green",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'ohrid', 'BDCT1', 'BDRS1', 'BDCT2', 'BDRS2', 'BDCT3', 'BDRS3','WDRS1', 'DCT1', 'DRS1', 'DCT2', 'DRS2', 'DCT3', 'DRS3'\n",
    "\n",
    "river_segments = ['ohrid', 'BDCT1', 'BDRS1', 'BDCT2', 'BDRS2', 'BDCT3', 'BDRS3','WDRS1', 'DCT1', 'DRS1', 'DCT2', 'DRS2', 'DCT3', 'DRS3']\n",
    "\n",
    "for river_segment in river_segments:\n",
    "    dff = dfts.loc[dfts['catchment']==river_segment]\n",
    "    graph_title = river_segment + ' historical flow' + ' -Reference Scenario'\n",
    "    fig = px.line(dff, x='Date', y='value', color='model',\n",
    "                  labels={\"value\": \"River discharge (m3/sec)\"}, title=graph_title,\n",
    "                  facet_col_spacing=0.05)\n",
    "    fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\n",
    "\n",
    "    #You can change the image extension to *.png if you want or keep it as pdf (for high resolution)\n",
    "    output_folder = os.path.join('processed_data','smhi_ref_graphs','Timeseries')\n",
    "    os.makedirs(output_folder, exist_ok = True)\n",
    "\n",
    "    #fig.write_image('processed_data/smhi_flow_graphs/REF/Timeseries/{}.png'.format(graph_title))\n",
    "    #fig.show()\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-nudist",
   "metadata": {},
   "source": [
    "### Weekly flow graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "alpha-reference",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'ohrid', 'BDCT1', 'BDRS1', 'BDCT2', 'BDRS2', 'BDCT3', 'BDRS3','WDRS1', 'DCT1', 'DRS1', 'DCT2', 'DRS2', 'DCT3', 'DRS3'\n",
    "\n",
    "river_segments = ['WDRS1', 'DCT1', 'DRS1', 'DCT2', 'DRS2', 'DCT3', 'DRS3']\n",
    "\n",
    "for river_segment in river_segments:\n",
    "    dff = dff_melted.loc[dff_melted['catchment']==river_segment]\n",
    "    graph_title = river_segment + ' weekly flow' + ' -Reference Scenario'\n",
    "    fig = px.line(dff, x='week', y='value', color='model',\n",
    "                  labels={\"value\": \"River discharge (m3/sec)\"}, title=graph_title,\n",
    "                  facet_col_spacing=0.05)\n",
    "    fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\n",
    "\n",
    "    #You can change the image extension to *.png if you want or keep it as pdf (for high resolution)\n",
    "    output_folder = os.path.join('processed_data','smhi_ref_graphs', 'Weekly')\n",
    "    os.makedirs(output_folder, exist_ok = True)\n",
    "\n",
    "    #fig.write_image('processed_data/smhi_flow_graphs/REF/Weekly/{}.png'.format(graph_title))\n",
    "    #fig.show()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-complex",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eastern-biotechnology",
   "metadata": {},
   "source": [
    "# Part 2: Climate change scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entertaining-holocaust",
   "metadata": {},
   "source": [
    "### 2.1 Calculating the average flow for each rcp scenario. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "spanish-inquiry",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = os.path.join('..','Data', 'smhi_data')\n",
    "scenario_path = os.path.join(folder ,'cc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "close-encounter",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-de601523eec7>:6: FutureWarning:\n",
      "\n",
      "weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "\n",
      "<ipython-input-20-de601523eec7>:30: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "<ipython-input-20-de601523eec7>:39: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "<ipython-input-20-de601523eec7>:47: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "<ipython-input-20-de601523eec7>:6: FutureWarning:\n",
      "\n",
      "weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "\n",
      "<ipython-input-20-de601523eec7>:30: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "<ipython-input-20-de601523eec7>:39: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "<ipython-input-20-de601523eec7>:47: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "<ipython-input-20-de601523eec7>:6: FutureWarning:\n",
      "\n",
      "weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "\n",
      "<ipython-input-20-de601523eec7>:30: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "<ipython-input-20-de601523eec7>:39: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "<ipython-input-20-de601523eec7>:47: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "<ipython-input-20-de601523eec7>:6: FutureWarning:\n",
      "\n",
      "weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "\n",
      "<ipython-input-20-de601523eec7>:30: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "<ipython-input-20-de601523eec7>:39: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "<ipython-input-20-de601523eec7>:47: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "<ipython-input-20-de601523eec7>:6: FutureWarning:\n",
      "\n",
      "weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "\n",
      "<ipython-input-20-de601523eec7>:30: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "<ipython-input-20-de601523eec7>:39: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "<ipython-input-20-de601523eec7>:47: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "<ipython-input-20-de601523eec7>:6: FutureWarning:\n",
      "\n",
      "weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "\n",
      "<ipython-input-20-de601523eec7>:30: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "<ipython-input-20-de601523eec7>:39: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "<ipython-input-20-de601523eec7>:47: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "<ipython-input-20-de601523eec7>:6: FutureWarning:\n",
      "\n",
      "weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "\n",
      "<ipython-input-20-de601523eec7>:30: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "<ipython-input-20-de601523eec7>:39: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "<ipython-input-20-de601523eec7>:47: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-de601523eec7>:6: FutureWarning:\n",
      "\n",
      "weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "\n",
      "<ipython-input-20-de601523eec7>:30: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "<ipython-input-20-de601523eec7>:39: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "<ipython-input-20-de601523eec7>:47: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for filepath in glob.iglob(os.path.join(scenario_path, '*.xls')):\n",
    "    #print(filepath)\n",
    "    df=pd.read_excel(filepath , sheet_name='E-HYPE312')\n",
    "    df['year'] = pd.DatetimeIndex(df['Date']).year\n",
    "    df['month']=pd.DatetimeIndex(df['Date']).month\n",
    "    df['week']=pd.DatetimeIndex(df['Date']).week\n",
    "    df['day']=pd.DatetimeIndex(df['Date']).day\n",
    "    df=df[(df['year']>=2020) & (df['year']<=2055)]\n",
    "    i=filepath[21:26]\n",
    "    df['catchment']= i\n",
    "    \n",
    "    # generating average flow for each rcp by taking the mean of different models outputs:\n",
    "    df2=df.copy()\n",
    "    df2=df.groupby(['year','week']).mean().round(3).reset_index()\n",
    "    df2['catchment']= i\n",
    "    \n",
    "    df2['RCP26']=df2[['CSC_REMO2009_MPI-ESM-LR_rcp26','SMHI_RCA4_EC-EARTH_rcp26']].mean(axis=1)\n",
    "    df2['RCP45']=df2[['CSC_REMO2009_MPI-ESM-LR_rcp45','KNMI_RACMO22E_EC-EARTH_rcp45','SMHI_RCA4_EC-EARTH_rcp45','SMHI_RCA4_HadGEM2-ES_rcp45','IPSL-IPSL-CM5A-MR_rcp45' ]].mean(axis=1)\n",
    "    df2['RCP85']=df2[['CSC_REMO2009_MPI-ESM-LR_rcp85','KNMI_RACMO22E_EC-EARTH_rcp85','SMHI_RCA4_EC-EARTH_rcp85','SMHI_RCA4_HadGEM2-ES_rcp85']].mean(axis=1)\n",
    "    df2=df2.drop(columns=['month', 'day','CSC_REMO2009_MPI-ESM-LR_rcp26',\n",
    "           'CSC_REMO2009_MPI-ESM-LR_rcp45', 'CSC_REMO2009_MPI-ESM-LR_rcp85',\n",
    "           'IPSL-IPSL-CM5A-MR_rcp45', 'KNMI_RACMO22E_EC-EARTH_rcp45',\n",
    "           'KNMI_RACMO22E_EC-EARTH_rcp85', 'SMHI_RCA4_EC-EARTH_rcp26',\n",
    "           'SMHI_RCA4_EC-EARTH_rcp45', 'SMHI_RCA4_EC-EARTH_rcp85',\n",
    "           'SMHI_RCA4_HadGEM2-ES_rcp45', 'SMHI_RCA4_HadGEM2-ES_rcp85'])\n",
    "\n",
    "   \n",
    "    #filtering by rcp and saving the files (Note: the missing catchments are not added yet)\n",
    "    df3 = df2[['catchment','year','week','RCP26']]\n",
    "    df3['scenario']='RCP26'\n",
    "    df3['model']='e_hype'\n",
    "    df3.rename({'RCP26':'value'}, axis=1, inplace=True)\n",
    "    rcp26folder = os.path.join('processed_data','smhi_cc_data','rcp26')\n",
    "    os.makedirs(rcp26folder, exist_ok = True)\n",
    "    df3.to_csv(os.path.join(rcp26folder,'rcp26_'+ i + '.csv'))\n",
    "    \n",
    "    \n",
    "    df4 = df2[['catchment','year','week','RCP45']]\n",
    "    df4['scenario']='RCP45'\n",
    "    df4['model']='e_hype'\n",
    "    df4.rename({'RCP45':'value'}, axis=1, inplace=True)\n",
    "    rcp45folder = os.path.join('processed_data','smhi_cc_data','rcp45')\n",
    "    os.makedirs(rcp45folder, exist_ok = True)\n",
    "    df4.to_csv(os.path.join(rcp45folder,'rcp45_'+ i + '.csv'))\n",
    "    \n",
    "    df5 = df2[['catchment','year','week','RCP85']]\n",
    "    df5['scenario']='RCP85'\n",
    "    df5['model']='e_hype'\n",
    "    df5.rename({'RCP85':'value'}, axis=1, inplace=True)\n",
    "    rcp85folder = os.path.join('processed_data','smhi_cc_data', 'rcp85')\n",
    "    os.makedirs(rcp85folder, exist_ok = True)\n",
    "    df5.to_csv(os.path.join(rcp85folder,'rcp85_'+ i + '.csv'))\n",
    "    \n",
    "    #combining the flow for all rcps:\n",
    "    df6 = df3.append([df4,df5])\n",
    "    df6 = df6[['model','catchment','scenario','year','week','value']]\n",
    "    allfolder = os.path.join('processed_data','smhi_cc_data','allrcps')\n",
    "    os.makedirs(allfolder, exist_ok = True)\n",
    "    df6.to_csv(os.path.join(allfolder,'all_'+ i + '.csv'))\n",
    "    \n",
    "    #print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-bedroom",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "precise-ownership",
   "metadata": {},
   "source": [
    "### 2.2 Restructuring the river segments and introducing the missing catchments under each rcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "novel-swaziland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This merge will add the missing cathcments and will save merge all catchments under each rcp\n",
    "# Make sure that you delete the old files, otherwise it might merge old runs as well.\n",
    "# The result is one rcp file for all catchments. \n",
    "\n",
    "rcps = ['rcp26', 'rcp45', 'rcp85']\n",
    "\n",
    "for rcp in rcps:\n",
    "    joined_files = os.path.join('processed_data','smhi_cc_data', rcp, \"rcp*.csv\")\n",
    "\n",
    "    # A list of all joined files is returned\n",
    "    joined_list = glob.glob(joined_files)\n",
    "\n",
    "    # The files are joined\n",
    "    df = pd.concat(map(pd.read_csv, joined_list), ignore_index=True)\n",
    "\n",
    "    df2 = pd.pivot_table(df, values='value', \n",
    "        index=[ 'model','scenario', 'year', 'week'], \n",
    "        columns=['catchment'], aggfunc=np.sum).round(3).reset_index()\n",
    "\n",
    "    df2.rename({\n",
    "    #'ohrid':'ohrid_ct',  #ohrid lake outflow\n",
    "    #'globo':'BDCT1',      #Inflow from Golbo catchment\n",
    "    'globo':'BDRS1',      #Inflow from Golbo catchment\n",
    "    'spilj':'BDRS2',      #Inflow in river segment before spilje\n",
    "    'black':'BDRS3',      #Inflow in the river segment after spilje and before white drin and skavica\n",
    "    'white':'WDRS1',      #Inflow from white drin\n",
    "    'fierz':'DRS1',       #Inflow in river segment before fierza HPP\n",
    "    'koman':'DRS2',       #Inflow in river segment before koman HPP\n",
    "    'vaudj':'DRS3'        #Inflow in river segment before Vau i dejes HPP\n",
    "    }, axis=1, inplace=True)\n",
    "\n",
    "    #df2[['ohrid', 'BDCT1', 'BDRS1']]\n",
    "#     df2['BDRS1'] = df2['ohrid'] + df2['BDCT1']  #Inflow in river segment before Globocica\n",
    "#     df2['BDCT2'] = df2['BDRS2'] - df2['BDRS1']  #Inflow from catchment after Globocica and before Spilje\n",
    "#     df2['BDCT3'] = df2['BDRS3'] - df2['BDRS2']  #Inflow from catchment after Spilje and before white drin \n",
    "#     df2['DCT1'] = df2['DRS1'] - (df2['BDRS3']+df2['WDRS1'])  #Inflow from catchment before Fierza \n",
    "#     df2['DCT2'] = df2['DRS2'] - df2['DRS1']                  #Inflow from catchment before Koman\n",
    "#     df2['DCT3'] = df2['DRS3'] - df2['DRS2']                  #Inflow from catchment before vau i dejes\n",
    "\n",
    "    years = df2['year']\n",
    "    for year in years:\n",
    "        #df2['BDRS1'] = df2['ohrid']+df2['BDCT1']  #Inflow in river segment before Globocica\n",
    "        df2.loc[df2['BDRS1']>df2['ohrid'], 'BDCT1'] = df2['BDRS1']- df2['ohrid']\n",
    "        df2.loc[df2['BDRS1']<=df2['ohrid'], 'BDCT1'] = df2['BDCT1'].min()\n",
    "        \n",
    "\n",
    "        #Inflow from catchment after Globocica and before Spilje\n",
    "        df2.loc[df2['BDRS2']>df2['BDRS1'], 'BDCT2'] = df2['BDRS2']-df2['BDRS1']\n",
    "        df2.loc[df2['BDRS2']<=df2['BDRS1'], 'BDCT2'] = df2['BDCT2'].min()\n",
    "\n",
    "        #Inflow from catchment after Spilje and before white drin \n",
    "        df2.loc[df2['BDRS3']>df2['BDRS2'], 'BDCT3'] = df2['BDRS3']-df2['BDRS2']\n",
    "        df2.loc[df2['BDRS3']<=df2['BDRS2'], 'BDCT3'] = df2['BDCT3'].min()\n",
    "\n",
    "\n",
    "        #Inflow from catchment before Fierza \n",
    "        df2.loc[df2['DRS1']>(df2['BDRS3']+df2['WDRS1']), 'DCT1'] = df2['DRS1']-(df2['BDRS3']+df2['WDRS1'])\n",
    "        df2.loc[df2['DRS1']<=(df2['BDRS3']+df2['WDRS1']), 'DCT1'] = df2['WDRS1'].min()\n",
    "\n",
    "        #Inflow from catchment before Koman\n",
    "        df2.loc[df2['DRS2']>df2['DRS1'], 'DCT2'] = df2['DRS2']-df2['DRS1']\n",
    "        df2.loc[df2['DRS2']<=df2['DRS1'], 'DCT2'] = df2['DCT2'].min()\n",
    "\n",
    "        #Inflow from catchment before vau i dejes\n",
    "        df2.loc[df2['DRS3']>df2['DRS2'], 'DCT3'] = df2['DRS3']-df2['DRS2']\n",
    "        df2.loc[df2['DRS3']<=df2['DRS2'], 'DCT3'] = df2['DCT3'].min()\n",
    "\n",
    "    \n",
    "    df2 = df2[['model','scenario', 'year', 'week','ohrid', 'BDCT1','BDRS1','BDCT2', 'BDRS2','BDCT3', 'BDRS3', 'WDRS1', 'DCT1','DRS1','DCT2', 'DRS2','DCT3','DRS3']]\n",
    "\n",
    "    dff = df2.copy()\n",
    "    dff_melted = dff.melt(id_vars=['model','scenario','year','week'], var_name='catchment',value_name='value',ignore_index=True)\n",
    "\n",
    "    dff_melted = dff_melted[['model','scenario','year','catchment','week','value']]\n",
    "\n",
    "    results_folder = os.path.join('processed_data','smhi_cc_data', 'adding_missing_catchments')\n",
    "    os.makedirs(results_folder, exist_ok = True)\n",
    "    dff_melted.to_csv(os.path.join(results_folder, 'adding_missing_catchments_' + rcp + '.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mental-sender",
   "metadata": {},
   "source": [
    "### 2.3 Merging the files for the three rcps into one final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "compliant-acrobat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this step we will merge the files for the three rcps in one final dataset\n",
    "\n",
    "#for rcp in rcps:\n",
    "joined_files = os.path.join('processed_data','smhi_cc_data', 'adding_missing_catchments', \"adding_missing_catchments_*.csv\")\n",
    "#print(joined_files)\n",
    "\n",
    "# A list of all joined files is returned\n",
    "joined_list = glob.glob(joined_files)\n",
    "#print(joined_list)\n",
    "\n",
    "# The files are joined\n",
    "df_all = pd.concat(map(pd.read_csv, joined_list), ignore_index=True)\n",
    "df_all.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "\n",
    "results_folder = os.path.join('processed_data','smhi_cc_data')\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "df_all.to_csv(os.path.join(results_folder, ('ehype_cc_allcatchments_allrcps_d.csv')),index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-capability",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "greek-photographer",
   "metadata": {},
   "source": [
    "# Part 3: Calculating river segments Capacity Factor (CF), Residual capacity and lower activity limit: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-victorian",
   "metadata": {},
   "source": [
    "## The capacity factor of each river segment is calculated based on:\n",
    "\n",
    "CF = (the projected average weekly flow) / (the historical max flow) \n",
    "\n",
    "Q: should we use the max historical value? or the max monthly value?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-strategy",
   "metadata": {},
   "source": [
    "## The residual capacity is calculated based on: \n",
    "\n",
    "The mean (max?) river flow for each catchment in each year * (3600 sec)*(8760 days) and divide by 1000000 to get the capacity in MCM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-israeli",
   "metadata": {},
   "source": [
    "## The lower Activity limit is based on: \n",
    "lowerlimit = the sum of the weekly flows in each year (average of several years). (this is the first approach)\n",
    "\n",
    "comment: \n",
    "This might result in over production from river segments.\n",
    "\n",
    "try changing this to: lowerlimit = weeklyflow.min() * 52 weeks. \n",
    "This means that we force the river segment to at least produce to the min flow. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "medical-capitol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the input files: \n",
    "input_folder = os.path.join('processed_data')\n",
    "df_ref = pd.read_csv(os.path.join(input_folder, 'smhi_ref_data', 'ehype_ref_weeklyavg_allcatchments_b20220211.csv'))\n",
    "df_cc = pd.read_csv(os.path.join(input_folder, 'smhi_cc_data','ehype_cc_allcatchments_allrcps_d.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "reduced-sandwich",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined script:\n",
    "# This scrip will take the input files per scenario (ref, cc) and calculate the capacity factor, residual capacity and lower activity limit\n",
    "# It will then save the files for each scenario, catchment, rcp and parameter:\n",
    "\n",
    "\n",
    "scenarios = ['REF', 'CC']\n",
    "\n",
    "for scenario in scenarios:\n",
    "    if scenario == 'REF':\n",
    "        df = df_ref\n",
    "    else:\n",
    "        df = df_cc\n",
    "    \n",
    "        \n",
    "# Deleting the segments that will not be used as inputs to osemosys:\n",
    "#     df = df.loc[(df['catchment']!='BDRS1')&(df['catchment']!='BDRS2')&(df['catchment']!='BDRS3')\n",
    "#       &(df['catchment']!='DRS1')&(df['catchment']!='DRS2')&(df['catchment']!='DRS3')]\n",
    "    \n",
    "    # Renaming the river segments to match osemosys structure:\n",
    "    df.set_index('catchment', inplace=True)\n",
    "\n",
    "    df.rename({'ohrid':'MKCWTLK0BD',\n",
    "               'DCT1':'ALCWTCT1DD',\n",
    "               'BDCT1':'MKCWTCT1BD',\n",
    "               'DCT2':'ALCWTCT2DD',\n",
    "               'Orhid':'MKCWTLK0BD',\n",
    "               'BDCT2':'MKCWTCT2BD',\n",
    "               'BDCT3':'MKCWTCT3DD',\n",
    "               'DCT3':'ALCWTCT3DD',\n",
    "               'WDRS1':'XKCWTRS1WD',\n",
    "               'BDRS1':'MKCWTRS1BD',\n",
    "               'BDRS2':'MKCWTRS2BD',\n",
    "               'BDRS3':'MKCWTRS3DD',\n",
    "               'DRS1':'ALCWTRS1DD',\n",
    "               'DRS2':'ALCWTRS2DD',\n",
    "               'DRS3':'ALCWTRS3DD'\n",
    "              }, axis=0, inplace=True)\n",
    "\n",
    "    df.reset_index(inplace=True)\n",
    "    \n",
    "    max_value = df_ref.loc[df_ref['week']!=53]\n",
    "    max_value = df_ref.groupby(['catchment'])['value'].max()\n",
    "    max_value.rename({'ohrid':'MKCWTLK0BD',\n",
    "                   'DCT1':'ALCWTCT1DD',\n",
    "                   'BDCT1':'MKCWTCT1BD',\n",
    "                   'DCT2':'ALCWTCT2DD',\n",
    "                   'Orhid':'MKCWTLK0BD',\n",
    "                   'BDCT2':'MKCWTCT2BD',\n",
    "                   'BDCT3':'MKCWTCT3DD',\n",
    "                   'DCT3':'ALCWTCT3DD',\n",
    "                   'WDRS1':'XKCWTRS1WD',\n",
    "                    'BDRS1':'MKCWTRS1BD',\n",
    "                   'BDRS2':'MKCWTRS2BD',\n",
    "                   'BDRS3':'MKCWTRS3DD',\n",
    "                   'DRS1':'ALCWTRS1DD',\n",
    "                   'DRS2':'ALCWTRS2DD',\n",
    "                   'DRS3':'ALCWTRS3DD'\n",
    "            }, axis=0, inplace=True)\n",
    "    #max_value.drop(['BDRS1', 'BDRS2', 'BDRS3','DRS1', 'DRS2', 'DRS3'], inplace=True) \n",
    "    \n",
    "    mean_value = df_ref.loc[df_ref['week']!=53]\n",
    "    mean_value = df_ref.groupby(['catchment'])['value'].mean()\n",
    "    mean_value.rename({'ohrid':'MKCWTLK0BD',\n",
    "                   'DCT1':'ALCWTCT1DD',\n",
    "                   'BDCT1':'MKCWTCT1BD',\n",
    "                   'DCT2':'ALCWTCT2DD',\n",
    "                   'Orhid':'MKCWTLK0BD',\n",
    "                   'BDCT2':'MKCWTCT2BD',\n",
    "                   'BDCT3':'MKCWTCT3DD',\n",
    "                   'DCT3':'ALCWTCT3DD',\n",
    "                   'WDRS1':'XKCWTRS1WD',\n",
    "                    'BDRS1':'MKCWTRS1BD',\n",
    "                   'BDRS2':'MKCWTRS2BD',\n",
    "                   'BDRS3':'MKCWTRS3DD',\n",
    "                   'DRS1':'ALCWTRS1DD',\n",
    "                   'DRS2':'ALCWTRS2DD',\n",
    "                   'DRS3':'ALCWTRS3DD'\n",
    "            }, axis=0, inplace=True)\n",
    "    #mean_value.drop(['BDRS1', 'BDRS2', 'BDRS3','DRS1', 'DRS2', 'DRS3'], inplace=True) \n",
    "    \n",
    "    \n",
    "    # Merge the projected values with the historical max for each river segment and calculate the CF:\n",
    "    res_cap = df.groupby(['catchment', 'scenario', 'year'])['value'].max()*(3600*8760/1000000)\n",
    "    lowlimit = df.groupby(['catchment','scenario','year'])['value'].min()*52\n",
    "    \n",
    "    \n",
    "    df2 = pd.merge(df, max_value, on=['catchment'])\n",
    "    df2.rename({'value_x':'proj_weekly_flow' ,\n",
    "                     'value_y':'hist_max_flow'}, axis=1, inplace=True)\n",
    "    df2['cf'] = df2['proj_weekly_flow']/df2['hist_max_flow']\n",
    "    \n",
    "    \n",
    "    #res capacity calculations: \n",
    "\n",
    "#     res_cap = df2.groupby(['catchment', 'scenario', 'year'])['proj_weekly_flow'].mean()*(3600*8760/1000000)\n",
    "\n",
    "    df3 = pd.merge(df2, res_cap, on=['catchment','scenario','year'])\n",
    "    \n",
    "    df3.rename({'value':'res_cap'\n",
    "                }, axis=1, inplace=True)\n",
    "    df3=df3.drop_duplicates()\n",
    "    \n",
    "#     #Lower Activity calculations: \n",
    "    #lowlimit = df3.groupby(['catchment','scenario','year'])['proj_weekly_flow'].sum()\n",
    "\n",
    "    df4 = pd.merge(df3, lowlimit, on=['catchment','scenario','year'])\n",
    "    df4.rename({'value':'loweractivitylimit'\n",
    "                }, axis=1, inplace=True)\n",
    "    df4=df4.drop_duplicates()\n",
    "\n",
    "\n",
    "###### SAVING THE OUTPUT FILES: #############\n",
    "\n",
    "    catchments = df4['catchment'].unique()\n",
    "    \n",
    "    if scenario == 'REF':\n",
    "        rcps = ['ref']\n",
    "    else:\n",
    "        rcps = ['RCP26','RCP45','RCP85']\n",
    "        \n",
    "        \n",
    "    for catchment in catchments:\n",
    "        for rcp in rcps:\n",
    "            output_folder = os.path.join('processed_data', 'final_outputs', scenario,'capacity_factors')\n",
    "            os.makedirs(output_folder, exist_ok = True)\n",
    "            dfcf=  df4.loc[df4['catchment']==catchment]\n",
    "            dfcf = dfcf.loc[dfcf['scenario']==rcp]\n",
    "            dfcf = dfcf.loc[dfcf['week']!=53]\n",
    "            dfcf = pd.pivot_table(dfcf, values='cf',\n",
    "                                 index=['scenario','catchment','week'],\n",
    "                                 columns=['year'], aggfunc=np.mean).round(3).reset_index()\n",
    "            \n",
    "            dfcf.to_csv(os.path.join(output_folder, 'Capacity_factors_' + rcp + '_' + catchment + '.csv'),index=False)\n",
    "    \n",
    "    \n",
    "    for rcp in rcps:\n",
    "        output_folder = os.path.join('processed_data', 'final_outputs', scenario,'res_capacity')\n",
    "        os.makedirs(output_folder, exist_ok = True)\n",
    "        #dff= df4.loc[df4['catchment']==catchment]\n",
    "        dfrc = df4.loc[df4['scenario']==rcp]\n",
    "        dfrc = dfrc.loc[dfrc['week']!=53]\n",
    "        dfrc = pd.pivot_table(dfrc, values='res_cap',\n",
    "                            index=['scenario', 'year'],\n",
    "                            columns=['catchment'], aggfunc=np.mean).round(3).reset_index()\n",
    "        #Adjusting the residual capacity values:\n",
    "        #Globocica\n",
    "        dfrc['MKCWTLK0BD']=dfrc.MKCWTLK0BD\n",
    "        dfrc['MKCWTCT1BD']=dfrc.MKCWTCT1BD\n",
    "        dfrc['MKCWTRS1BD']=dfrc['MKCWTLK0BD']+dfrc['MKCWTCT1BD']\n",
    "        #dfrc['MKCWTSP1BD'] = \n",
    "\n",
    "        #Shpilje\n",
    "        dfrc['MKCWTCT2BD']=dfrc.MKCWTCT2BD\n",
    "        dfrc['MKCWTRS2BD']=dfrc['MKCWTRS1BD']+dfrc['MKCWTCT2BD']\n",
    "        #dfrc['MKCWTSP2BD'] =\n",
    "\n",
    "        #Skavica\n",
    "        dfrc['MKCWTCT3DD']=dfrc.MKCWTCT3DD\n",
    "        dfrc['MKCWTRS3DD']=dfrc['MKCWTRS2BD']+dfrc['MKCWTCT3DD']\n",
    "        #dfrc['ALCWTSP3DD'] =\n",
    "\n",
    "\n",
    "        #White_Drin\n",
    "        dfrc['XKCWTRS1WD']=dfrc.XKCWTRS1WD\n",
    "\n",
    "        #Fierza\n",
    "        dfrc['ALCWTCT1DD']=dfrc.ALCWTCT1DD\n",
    "        dfrc['ALCWTRS1DD']=dfrc['MKCWTRS3DD']+ dfrc['XKCWTRS1WD']+dfrc['ALCWTCT1DD']\n",
    "        #dfrc['ALCWTSP4DD'] =\n",
    "\n",
    "        #Koman\n",
    "        dfrc['ALCWTCT2DD']=dfrc.ALCWTCT2DD\n",
    "        dfrc['ALCWTRS2DD']=dfrc['ALCWTRS1DD']+ dfrc['ALCWTCT2DD']\n",
    "        #dfrc['ALCWTSP5DD'] =\n",
    "\n",
    "        #Vau_dejas\n",
    "        dfrc['ALCWTCT3DD']=dfrc.ALCWTCT3DD\n",
    "        dfrc['ALCWTRS3DD']=dfrc['ALCWTRS2DD']+ dfrc['ALCWTCT3DD']\n",
    "        #dfrc['ALCWTSP6DD'] =\n",
    "\n",
    "        #Drin_outflow\n",
    "        #dfrc['ALCWTCT4DD']=dfrc.ALCWTCT4DD\n",
    "       \n",
    "    \n",
    "        dfrc = dfrc.T.reset_index()\n",
    "        dfrc.columns = dfrc.iloc[1]\n",
    "        dfrc = dfrc.drop(labels=[0,1], axis=0)\n",
    "        \n",
    "#         for i in range(20,56):\n",
    "#             if scenario =='REF':\n",
    "#                 dfrc['20{}'.format(i)] = dfrc[2020]\n",
    "#             else:\n",
    "#                 dfrc['20'{}'.format(i)]\n",
    "#         #dfrc.drop(2020, axis=1, inplace=True)\n",
    "        dfrc.to_csv(os.path.join(output_folder, 'res_capacity_' + rcp + '.csv'),index=False)\n",
    "            \n",
    "    \n",
    "    for rcp in rcps:\n",
    "        output_folder = os.path.join('processed_data', 'final_outputs', scenario,'loweractivity')\n",
    "        os.makedirs(output_folder, exist_ok = True)\n",
    "        #dff= df4.loc[df4['catchment']==catchment]\n",
    "        dfla = df4.loc[df4['scenario']==rcp]\n",
    "        dfla = dfla.loc[dfla['week']!=53]\n",
    "        dfla = pd.pivot_table(dfla, values='loweractivitylimit',\n",
    "                             index=['scenario','catchment'],\n",
    "                             columns=['year'], aggfunc=np.mean).round(3).reset_index()\n",
    "\n",
    "        dfla.to_csv(os.path.join(output_folder, 'loweractivity_' + rcp + '.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "quarterly-afghanistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post processing of the CF dataframes for the REF SCENARIO ONLY:\n",
    "\n",
    "\n",
    "scenarios = ['REF']\n",
    "\n",
    "for scenario in scenarios:\n",
    "    input_folder = os.path.join('processed_data', 'final_outputs', scenario,'capacity_factors')\n",
    "\n",
    "    for filepath in glob.iglob(os.path.join(input_folder, ('*.csv'))):\n",
    "        dfcf2=pd.read_csv(filepath)\n",
    "        name = filepath[71:81]  #check if the naming is correct, else adjust this\n",
    "        \n",
    "#        print(filepath)\n",
    "\n",
    "        for i in range(20,56):\n",
    "            if scenario =='REF':\n",
    "                dfcf2['20{}'.format(i)] = dfcf2['2020']\n",
    "            else:\n",
    "                dfcf2['20{}'.format(i)]\n",
    "            output_folder = os.path.join('processed_data', 'final_outputs', scenario,'capacity_factors2')\n",
    "            os.makedirs(output_folder, exist_ok= True)\n",
    "            dfcf2.to_csv(os.path.join(output_folder,('CapacityFactor2_'+str(name)+'.csv')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-daily",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-privilege",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
